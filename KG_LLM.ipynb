{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4-Qbqh3B1o0"
      },
      "source": [
        "# **KG-LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ8DbjoLYO5c",
        "outputId": "136993fc-3e26-4fe8-cbbb-109890a8dc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW5z5KbusKMN"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI91YyWsxleS"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install --upgrade pyarrow\n",
        "!pip install --upgrade pandas datasets\n",
        "!pip install sentencepiece\n",
        "!pip install sentencepiece --force-reinstall\n",
        "!pip install -U scikit-learn\n",
        "!pip install torch\n",
        "!pip install accelerate\n",
        "!pip install datasets==2.13.1\n",
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install trl\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgoJZSnHF_Xo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from collections import deque\n",
        "import csv\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nVNkdbiylW9"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESX6yoZN-DLj"
      },
      "source": [
        "# *KG-LLM(CoT)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8536ILChsQM"
      },
      "source": [
        "**Preprocess data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIaibsfsMwUn"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_file = open(\"train2id.txt\", \"r\")\n",
        "output_file = open(\"output.txt\", \"w\")\n",
        "\n",
        "# total number of lines\n",
        "number = int(input_file.readline())\n",
        "\n",
        "nodes = set()\n",
        "\n",
        "graph = {}\n",
        "\n",
        "for i in range(number):\n",
        "    content = input_file.readline()\n",
        "    node1, node2, relation = content.strip().split()\n",
        "\n",
        "    nodes.add(node1)\n",
        "    # nodes.add(node2)\n",
        "\n",
        "    relation = int(relation)\n",
        "\n",
        "    # Check if the first node already exists in the dictionary\n",
        "    if node1 not in graph:\n",
        "        # If not, create a new dictionary for the node\n",
        "        graph[node1] = {}\n",
        "    # Add the neighboring node and the relationship to the dictionary for node1\n",
        "    graph[node1][node2] = relation\n",
        "\n",
        "\n",
        "node_list = list(nodes)\n",
        "node_list2 = list(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TwhNMrw9qD3",
        "outputId": "1fcd2f4f-19ff-4a0e-e48e-8355300e1854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "relation2id = {}\n",
        "\n",
        "with open(\"relation2id.txt\", \"r\") as file:\n",
        "    relations = int(file.readline())\n",
        "    for line in file:\n",
        "        relation, relation_id = line.strip().split(\"\\t\")\n",
        "        relation2id[int(relation_id)] = relation\n",
        "\n",
        "unique_rows = set()\n",
        "\n",
        "# change size if you want, but larger the size, smaller the data\n",
        "size = 30\n",
        "# how many positive and negative data set you want to train, the more dataset, the more time to train\n",
        "total = 60000\n",
        "fieldnames = ['Prompt', 'input_text', 'output_text']\n",
        "\n",
        "instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "\n",
        "with open(\"train_data.csv\", mode=\"w\", newline='') as tra:\n",
        "  with open(\"positive_data.csv\", mode=\"w\", newline='') as pos:\n",
        "    with open(\"negative_data.csv\",  mode=\"w\", newline='') as neg:\n",
        "\n",
        "        # Create a CSV writer object and write the headers to the file\n",
        "        writer_pos = csv.DictWriter(pos, fieldnames=fieldnames)\n",
        "        writer_pos.writeheader()\n",
        "\n",
        "        writer_neg = csv.DictWriter(neg, fieldnames=fieldnames)\n",
        "        writer_neg.writeheader()\n",
        "\n",
        "        writer_tra = csv.DictWriter(tra, fieldnames=fieldnames)\n",
        "        writer_tra.writeheader()\n",
        "\n",
        "\n",
        "        def dfs(graph, size):\n",
        "            pos_count = 0\n",
        "            neg_count = 0\n",
        "            times = 0\n",
        "            term = True\n",
        "\n",
        "            while times < total:\n",
        "                visited = set()\n",
        "                kg = []\n",
        "                graph_size = random.randint(2, size)\n",
        "                first_node = random.choice(node_list)\n",
        "                visited.add(first_node)\n",
        "                last_node = \"\"\n",
        "                previous_node = first_node\n",
        "                stack = [first_node]\n",
        "                input_text = \"\"\n",
        "                output_text = \"\"\n",
        "                while len(visited) < graph_size:\n",
        "                    if previous_node not in graph or set(graph[previous_node].keys()).issubset(visited):\n",
        "                        node = random.choice(node_list)\n",
        "                        while node in visited:\n",
        "                            node = random.choice(node_list)\n",
        "                        input_text += 'node_{} not connected with node_{}. '.format(previous_node, node)\n",
        "                        output_text += 'node_{} not connected with node_{} means there is no relationship between node_{} and node_{}. '.format(previous_node, node, previous_node, node)\n",
        "                        visited.add(node)\n",
        "                        previous_node = node\n",
        "                    else:\n",
        "                        node = random.choice(list(graph[previous_node].keys()))\n",
        "                        while node in visited:\n",
        "                            node = random.choice(list(graph[previous_node].keys()))\n",
        "                        relation = graph[previous_node][node]\n",
        "                        r = relation2id[relation]\n",
        "                        input_text += 'node_{} has relation_{} with node_{}. '.format(previous_node, relation, node)\n",
        "                        output_text += 'node_{} has relation_{} with node_{}, means node_{} {} node_{}. '.format(previous_node, relation, node, previous_node, r, node)\n",
        "                        visited.add(node)\n",
        "                        previous_node = node\n",
        "                    if len(visited) == graph_size:\n",
        "                        last_node = previous_node\n",
        "\n",
        "                # input_text += 'Answer the following yes/no question by reasoning step-by-step. Is the first node connnected with the last node?'\n",
        "                was = len(unique_rows)\n",
        "                unique_rows.add(input_text)\n",
        "                if len(unique_rows) > was:\n",
        "                  # if pos_count < int(total/2):\n",
        "                    if first_node in graph and last_node in graph[first_node] and term:\n",
        "                      # if pos_count < int(total/2):\n",
        "                        final_relation = relation2id[graph[first_node][last_node]]\n",
        "                        output_text += 'So node {} {} node {}. The answer is yes.'.format(first_node, final_relation, last_node)\n",
        "                        prompt = 'Is node {} connnected with node {}?'.format(first_node, last_node)\n",
        "                        comb = \"###Instruction:\\n\" + instruction + prompt + \"\\n\\n###Input:\\n\" + input_text + \"\\n\\n###Response:\\n\" + output_text\n",
        "                        writer_pos.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        pos_count += 1\n",
        "                        term = False\n",
        "                        times += 1\n",
        "                    elif last_node in graph and first_node in graph[last_node] and term:\n",
        "                      # if pos_count < int(total/2):\n",
        "                        final_relation = relation2id[graph[last_node][first_node]]\n",
        "                        output_text += 'So node {} {} node {}. The answer is yes.'.format(last_node, final_relation, first_node)\n",
        "                        prompt = 'Is node {} connected with node {}?'.format(last_node, first_node)\n",
        "                        comb = \"###Instruction:\\n\" + instruction + prompt + \"\\n\\n###Input:\\n\" + input_text + \"\\n\\n###Response:\\n\" + output_text\n",
        "                        writer_pos.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        pos_count += 1\n",
        "                        term = False\n",
        "                        times += 1\n",
        "                  # elif neg_count < int(total//2):\n",
        "                    elif not term:\n",
        "                      # if neg_count < int(total/2):\n",
        "                        output_text += 'So there is no connection between node {} and node {}. The answer is no.'.format(first_node, last_node)\n",
        "                        prompt = 'Is node {} connected with node {}?'.format(first_node, last_node)\n",
        "                        comb = \"###Instruction:\\n\" + instruction + prompt + \"\\n\\n###Input:\\n\" + input_text + \"\\n\\n###Response:\\n\" + output_text\n",
        "                        writer_neg.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        neg_count += 1\n",
        "                        term = True\n",
        "                        times += 1\n",
        "                else:\n",
        "                  continue\n",
        "\n",
        "\n",
        "            print(pos_count)\n",
        "            print(neg_count)\n",
        "\n",
        "        dfs(graph, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "895ZvJ8MUhWD"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('train_data.csv')\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyt3pNhcQNcX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "positive_df = pd.read_csv('positive_data.csv')\n",
        "negative_df = pd.read_csv('negative_data.csv')\n",
        "\n",
        "train_pos, test_pos = train_test_split(positive_df, test_size=0.2, random_state=42)\n",
        "train_neg, test_neg = train_test_split(negative_df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = pd.concat([train_pos, train_neg])\n",
        "test_df = pd.concat([test_pos, test_neg])\n",
        "\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHHJdXC17UTT"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('train.csv')\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_JFDU6rlKpD"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5vR_fbdZkcp"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "#TODO:\n",
        "# !huggingface-cli login\n",
        "login(token = \"\")\n",
        "\n",
        "\n",
        "def load_model(model_name, bnb_config):\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{40960}MB'\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
        "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def preprocess_batch(batch, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        batch[\"Prompt\"],\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_bnb_config():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    return bnb_config\n",
        "\n",
        "def create_peft_config(modules):\n",
        "    \"\"\"\n",
        "    Create Parameter-Efficient Fine-Tuning config for your model\n",
        "    :param modules: Names of the modules to apply Lora to\n",
        "    \"\"\"\n",
        "    config = LoraConfig(\n",
        "        r=16,  # dimension of the updated matrices\n",
        "        lora_alpha=64,  # parameter for scaling\n",
        "        target_modules=modules,\n",
        "        lora_dropout=0.1,  # dropout probability for layers\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model, use_4bit=False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "        # if using DS Zero 3 and the weights are initialized empty\n",
        "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "            num_params = param.ds_numel\n",
        "\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "    if use_4bit:\n",
        "        trainable_params /= 2\n",
        "    print(\n",
        "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "def train(model, tokenizer, dataset, output_dir):\n",
        "    # Apply preprocessing to the model to prepare it by\n",
        "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Get lora module names\n",
        "    modules = find_all_linear_names(model)\n",
        "\n",
        "    # Create PEFT config for these modules and wrap the model to PEFT\n",
        "    peft_config = create_peft_config(modules)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print information about the percentage of trainable parameters\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Training parameters\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=2,\n",
        "            max_steps=15,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            logging_steps=1,\n",
        "            output_dir=\"outputs\",\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "        ),\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
        "\n",
        "\n",
        "    dtypes = {}\n",
        "    for _, p in model.named_parameters():\n",
        "        dtype = p.dtype\n",
        "        if dtype not in dtypes: dtypes[dtype] = 0\n",
        "        dtypes[dtype] += p.numel()\n",
        "    total = 0\n",
        "    for k, v in dtypes.items(): total+= v\n",
        "    for k, v in dtypes.items():\n",
        "        print(k, v, v/total)\n",
        "\n",
        "    do_train = True\n",
        "\n",
        "    # Launch training\n",
        "    print(\"Training...\")\n",
        "\n",
        "    if do_train:\n",
        "        train_result = trainer.train()\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        print(metrics)\n",
        "\n",
        "    ###\n",
        "\n",
        "    # Saving model\n",
        "    print(\"Saving last checkpoint of the model...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    trainer.model.save_pretrained(output_dir)\n",
        "\n",
        "    # Free memory for merging weights\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# TODO 这里file_path改成自己的csv 确保你的数据在Prompt的column里\n",
        "def main(file_path = \"/content/train.csv\", output_dir = \"final_checkpoint\"):\n",
        "    #Models that need to be fine-tuned\n",
        "    model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "    bnb_config = create_bnb_config()\n",
        "    model, tokenizer = load_model(model_name, bnb_config)\n",
        "    df = pd.read_csv(file_path)\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    _preprocessing_function = partial(preprocess_batch, tokenizer=tokenizer)\n",
        "\n",
        "    dataset = dataset.map(\n",
        "            _preprocessing_function,\n",
        "            batched=True,\n",
        "        )\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "    output_dir = output_dir\n",
        "    train(model, tokenizer, dataset, output_dir)\n",
        "\n",
        "main(\"/content/train.csv\", \"final_checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ADsTKjiarsx"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"### Instruction:\n",
        "Is the first node connect with the last node?\n",
        "\n",
        "###Input:\n",
        "node1 has relation_1 with node2, node2 has relation_2 with node 3\n",
        "\n",
        "###Response:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Specify device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Get answer\n",
        "# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Decode output & print it\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A-GcdRKAz6_"
      },
      "outputs": [],
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(\"final_checkpoint\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "output_merged_dir = \"results/llama2/final_merged_checkpoint\"\n",
        "os.makedirs(output_merged_dir, exist_ok=True)\n",
        "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
        "\n",
        "# save tokenizer for easy inference\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI6_wHlyCLii"
      },
      "outputs": [],
      "source": [
        "!zip -r CoTlink.zip /content/results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQn38VSEtJr-"
      },
      "source": [
        "**Link Prediction without ICL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4KUNNcm8gHg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 加载CSV文件\n",
        "csv_file = 'test.csv'  # 请替换为你的CSV文件名\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 在所有的'input_text'数据前面加上\"###Input:\"\n",
        "df['input_text'] = \"###Input: \\n\" + df['input_text']\n",
        "\n",
        "# 保存更改后的DataFrame到新的CSV文件\n",
        "df.to_csv('modified_' + csv_file, index=False)\n",
        "\n",
        "print(\"Updated CSV file has been saved as 'modified_\" + csv_file + \"'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIPgIfU3HV_C"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # 加载CSV文件\n",
        "# csv_file = 'modified_test.csv'  # 替换为你的CSV文件名\n",
        "# df = pd.read_csv(csv_file)\n",
        "\n",
        "# reference_text = \"###Input: node_60922 not connected with node_14564. node_60922 not connected with node_14564. node_60922 not connected with node_14564. node_14564 has relation_131 with node_17958. node_17958 has relation_6 with node_17474. Is node 66972 connected with node 17474?\"\n",
        "# reference_length = len(reference_text)\n",
        "\n",
        "# # 选择长度小于等于reference_text长度的行\n",
        "# filtered_df = df[df['input_text'].str.len() <= reference_length]\n",
        "\n",
        "# # 保存或处理过滤后的DataFrame\n",
        "# filtered_df.to_csv('filtered_' + csv_file, index=False)  # 如果需要，保存到新的CSV文件\n",
        "# num_rows = len(filtered_df)\n",
        "# print(f\"The new CSV file will have {num_rows} rows.\")\n",
        "\n",
        "# df1 = pd.read_csv('filtered_modified_test.csv')\n",
        "# df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0jSsmB4kZmj"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"###Input:\n",
        "node_58627 has relation_86 with node_1290. node_1290 has relation_2 with node_18074. node_18074 has relation_2 with node_14699. node_14699 has relation_2 with node_18079. Is node 58627 connected with node 18079?\"\"\"\n",
        "\n",
        "# Specify device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Get answer\n",
        "# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=130, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Decode output & print it\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1L36gNHmDd7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 读取CSV文件\n",
        "csv_file = '/content/modified_test.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 限制测试数量\n",
        "test_limit = 2000\n",
        "# 计数器\n",
        "accurate_count = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if index <= test_limit:  # 检查是否已达到测试数量的限制\n",
        "          break\n",
        "    # 获取输入文本\n",
        "    input_text = row['input_text']\n",
        "\n",
        "    # 使用模型生成回答\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
        "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # 检查模型回答和期望输出中是否包含'yes'或'Yes'\n",
        "    model_has_yes = 'yes' in model_answer.lower()\n",
        "    expected_has_yes = 'yes' in row['output_text'].lower()\n",
        "\n",
        "    # print(\"model_has_yes: \", model_has_yes)\n",
        "    # print(\"expected_has_yes: \", expected_has_yes)\n",
        "\n",
        "    y_true.append(expected_has_yes)\n",
        "    y_pred.append(model_has_yes)\n",
        "\n",
        "\n",
        "    # 判断准确性\n",
        "    if model_has_yes == expected_has_yes:\n",
        "        accurate_count += 1\n",
        "        print(accurate_count)\n",
        "\n",
        "    print(index)\n",
        "\n",
        "# 计算准确率\n",
        "f1 = f1_score(y_true, y_pred, pos_label=True)\n",
        "accuracy = accurate_count / min(len(df), test_limit)\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZODn_TD_RPK",
        "outputId": "3cd7c433-4769-4ec4-f2f0-cea17719404b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.953\n",
            "F1 Score: 0.975\n"
          ]
        }
      ],
      "source": [
        "f1 = f1_score(y_true, y_pred, pos_label=True)\n",
        "accuracy = accurate_count / min(len(df), test_limit)\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1fgo8i5HZ6"
      },
      "source": [
        "**Link Prediction with ICL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAAkAoP75eno"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('test.csv')\n",
        "count = len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB9kNLmi5MTV"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"train.csv\") as train:\n",
        "  with open(\"context.csv\", mode=\"w\", newline='') as context:\n",
        "      writer_context = csv.DictWriter(context, fieldnames=fieldnames)\n",
        "      writer_context.writeheader()\n",
        "      reader_train = csv.reader(train)\n",
        "      # Iterate over each row in the CSV file\n",
        "      for row in reader_train:\n",
        "          input = row[1]\n",
        "          output = row[2]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "\n",
        "          input = input.split('.')\n",
        "          input = input[:-1]\n",
        "          input = '.'.join(input)\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3:-1]\n",
        "          # relationship = words[3]\n",
        "          # print(words)\n",
        "          words = '.'.join(words)\n",
        "\n",
        "          if c < count and len(input) < 30:\n",
        "            cont = '###Context: {}.\\n\\n###Input:'.format(input + '.' + words)\n",
        "            writer_context.writerow({'input_text': cont, 'output_text': words})\n",
        "            c += 1\n",
        "            # print(c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FOR_9Q4d-eE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# 加载CSV文件\n",
        "context_df = pd.read_csv('context.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# 对test.csv中的每个input_text随机添加一个context.csv中的input_text\n",
        "for index, row in test_df.iterrows():\n",
        "    # 随机选择一个context\n",
        "    random_context = random.choice(context_df['input_text'])\n",
        "    # 将context和test中的文本合并，并直接覆盖原有的input_text\n",
        "    test_df.at[index, 'input_text'] = random_context + \" \" + row['input_text']\n",
        "\n",
        "# 保存更新后的test_df到CSV文件\n",
        "test_df.to_csv('augmented_test.csv', index=False)\n",
        "\n",
        "print(\"Augmented test CSV file has been created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQmovJCw85ay"
      },
      "outputs": [],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('augmented_test.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v55AOIQpNUH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 读取CSV文件\n",
        "csv_file = '/content/augmented_test.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 限制测试数量\n",
        "test_limit = 2000\n",
        "# 计数器\n",
        "accurate_count = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if index <= test_limit:  # 检查是否已达到测试数量的限制\n",
        "          break\n",
        "    # 获取输入文本\n",
        "    input_text = row['input_text']\n",
        "\n",
        "    # 使用模型生成回答\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
        "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # 检查模型回答和期望输出中是否包含'yes'或'Yes'\n",
        "    model_has_yes = 'yes' in model_answer.lower()\n",
        "    expected_has_yes = 'yes' in row['output_text'].lower()\n",
        "\n",
        "    # print(\"model_has_yes: \", model_has_yes)\n",
        "    # print(\"expected_has_yes: \", expected_has_yes)\n",
        "\n",
        "    y_true.append(expected_has_yes)\n",
        "    y_pred.append(model_has_yes)\n",
        "\n",
        "\n",
        "    # 判断准确性\n",
        "    if model_has_yes == expected_has_yes:\n",
        "        accurate_count += 1\n",
        "        print(accurate_count)\n",
        "\n",
        "    print(index)\n",
        "\n",
        "# 计算准确率\n",
        "f1 = f1_score(y_true, y_pred, pos_label=True)\n",
        "accuracy = accurate_count / min(len(df), test_limit)\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-oX8PaNpNUJ"
      },
      "outputs": [],
      "source": [
        "f1 = f1_score(y_true, y_pred, pos_label=True)\n",
        "accuracy = accurate_count / min(len(df), test_limit)\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNYQd-IfbrUS"
      },
      "source": [
        "# *Relation prediction without In-context learning*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC8eNXmRbtE4"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "# context = 'Given node_2075 has relation_4 with node_12648. The relation between the first node and last node is _member_meronym. '\n",
        "# instruction = 'Answer the following multiple-choice question by choosing one of these options: '\n",
        "# option = ''\n",
        "instruction = 'Answer the following question step-by-step. '\n",
        "count = 0\n",
        "\n",
        "question = 'The relationship between the first node and the last node is ?'\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"relation2id.txt\", \"r\") as file:\n",
        "    relations = int(file.readline())\n",
        "    for line in file:\n",
        "        relation, relation_id = line.strip().split(\"\\t\")\n",
        "    #     instruction += relation + ', '\n",
        "    # instruction = instruction[:-2] + '. '\n",
        "\n",
        "with open(\"test.csv\") as test:\n",
        "    with open(\"relation.csv\", mode=\"w\", newline='') as icl:\n",
        "      writer_icl = csv.DictWriter(icl, fieldnames=fieldnames)\n",
        "      writer_icl.writeheader()\n",
        "      reader = csv.reader(test)\n",
        "\n",
        "\n",
        "      for row in reader:\n",
        "          # print(row)\n",
        "          input = row[1]\n",
        "          output = row[2]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "          input = input.split('.')\n",
        "          input = input[:-1]\n",
        "          input = '.'.join(input)\n",
        "          # print(input)\n",
        "          input = instruction + input + '. ' + question\n",
        "          # input = input + '. ' + instruction + question\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "          # print(relationship)\n",
        "          output = output[:-2]\n",
        "          output = '.'.join(output)\n",
        "          # print(output)\n",
        "          output += '. The relationship between the first node and the last node is {}.'.format(relationship)\n",
        "          # print(output)\n",
        "          if len(input) < 550:\n",
        "            writer_icl.writerow({'input_text': input, 'output_text': output})\n",
        "            count+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej-meY0Hb4X-"
      },
      "outputs": [],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('relation.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLp8Y1c6b-8P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "csv_file = '/content/relation.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 初始化计数器和结果列表\n",
        "tp = fp = fn = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "test_limit = 500\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if index <= test_limit:  # 检查是否已达到测试数量的限制\n",
        "        break\n",
        "    input_text = row['input_text']\n",
        "    expected_word = row['output_text'].rstrip('.').split()[-1].lower()\n",
        "\n",
        "    # 使用模型生成回答\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
        "\n",
        "    # 检查模型回答中是否包含最后一个单词\n",
        "    contains_word = expected_word in model_answer.rstrip('.').split()\n",
        "\n",
        "    # 更新计数器\n",
        "    if contains_word:\n",
        "        tp += 1\n",
        "        y_pred.append(1)\n",
        "    else:\n",
        "        fn += 1\n",
        "        y_pred.append(0)\n",
        "    y_true.append(1)\n",
        "\n",
        "# 计算准确度和F1分数\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7x9nXYpb9os"
      },
      "source": [
        "# *Relation prediction with In-context learning*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddDmuyZhp-WO"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "# context = 'Given node_2075 has relation_4 with node_12648. The relation between the first node and last node is _member_meronym. '\n",
        "# instruction = 'Answer the following multiple-choice question by choosing one of these options: '\n",
        "# option = ''\n",
        "instruction = 'Answer the following question step-by-step. '\n",
        "count = 0\n",
        "\n",
        "question = 'The relationship between the first node and the last node is ?'\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"relation2id.txt\", \"r\") as file:\n",
        "    relations = int(file.readline())\n",
        "    for line in file:\n",
        "        relation, relation_id = line.strip().split(\"\\t\")\n",
        "    #     instruction += relation + ', '\n",
        "    # instruction = instruction[:-2] + '. '\n",
        "\n",
        "with open(\"test.csv\") as test:\n",
        "    with open(\"test_icl.csv\", mode=\"w\", newline='') as icl:\n",
        "      writer_icl = csv.DictWriter(icl, fieldnames=fieldnames)\n",
        "      writer_icl.writeheader()\n",
        "      reader = csv.reader(test)\n",
        "\n",
        "\n",
        "      for row in reader:\n",
        "          # print(row)\n",
        "          input = row[1]\n",
        "          output = row[2]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "          input = input.split('.')\n",
        "          input = input[:-1]\n",
        "          input = '.'.join(input)\n",
        "          # print(input)\n",
        "          input = instruction + input + '. ' + question\n",
        "          # input = input + '. ' + instruction + question\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "          # print(relationship)\n",
        "          output = output[:-2]\n",
        "          output = '.'.join(output)\n",
        "          # print(output)\n",
        "          output += '. The relationship between the first node and the last node is {}.'.format(relationship)\n",
        "          # print(output)\n",
        "          if len(input) < 550:\n",
        "            writer_icl.writerow({'input_text': input, 'output_text': output})\n",
        "            count+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvw4wJx803GZ"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "\n",
        "with open(\"train.csv\") as train:\n",
        "  with open(\"context.csv\", mode=\"w\", newline='') as context:\n",
        "      writer_context = csv.DictWriter(context, fieldnames=fieldnames)\n",
        "      writer_context.writeheader()\n",
        "      reader_train = csv.reader(train)\n",
        "      # Iterate over each row in the CSV file\n",
        "      for row in reader_train:\n",
        "          input = row[1]\n",
        "          output = row[2]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "\n",
        "          input = input.split('.')\n",
        "          input = input[:-1]\n",
        "          input = '.'.join(input)\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "\n",
        "          if c < count and len(input) < 50:\n",
        "            cont = '###Context: {}. The relationship between the first node and last node is {}.\\n\\n###Input:'.format(input, relationship)\n",
        "            writer_context.writerow({'input_text': cont, 'output_text': relationship})\n",
        "            c += 1\n",
        "            # print(c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wym7aI-yMb-y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# 加载CSV文件\n",
        "context_df = pd.read_csv('context.csv')\n",
        "test_df = pd.read_csv('test_icl.csv')\n",
        "\n",
        "# 对test.csv中的每个input_text随机添加一个context.csv中的input_text\n",
        "for index, row in test_df.iterrows():\n",
        "    # 随机选择一个context\n",
        "    random_context = random.choice(context_df['input_text'])\n",
        "    # 将context和test中的文本合并，并直接覆盖原有的input_text\n",
        "    test_df.at[index, 'input_text'] = random_context + \" \" + row['input_text']\n",
        "\n",
        "# 保存更新后的test_df到CSV文件\n",
        "test_df.to_csv('icl_relation.csv', index=False)\n",
        "\n",
        "print(\"Augmented test CSV file has been created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFcFffFgCrc1"
      },
      "outputs": [],
      "source": [
        "# read csv files\n",
        "df1 = pd.read_csv('context.csv')\n",
        "df2 = pd.read_csv('test_icl.csv')\n",
        "\n",
        "# combine dataframes by interleaving rows\n",
        "\n",
        "combined_df = pd.concat([df1, df2]).sort_index(kind='merge')\n",
        "\n",
        "# write to new csv file\n",
        "combined_df.to_csv('icl_relation.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51xxUqU7AEMV"
      },
      "outputs": [],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('icl_relation.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBWJWOWDMrCD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "csv_file = '/content/icl_relation.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 初始化计数器和结果列表\n",
        "tp = fp = fn = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "test_limit = 500\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if index <= test_limit:  # 检查是否已达到测试数量的限制\n",
        "        break\n",
        "    input_text = row['input_text']\n",
        "    expected_word = row['output_text'].rstrip('.').split()[-1].lower()\n",
        "\n",
        "    # 使用模型生成回答\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
        "\n",
        "    # 检查模型回答中是否包含最后一个单词\n",
        "    contains_word = expected_word in model_answer.rstrip('.').split()\n",
        "\n",
        "    # 更新计数器\n",
        "    if contains_word:\n",
        "        tp += 1\n",
        "        y_pred.append(1)\n",
        "    else:\n",
        "        fn += 1\n",
        "        y_pred.append(0)\n",
        "    y_true.append(1)\n",
        "\n",
        "# 计算准确度和F1分数\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpEDrGRzCEq7"
      },
      "source": [
        "# *KG-LLM(Standard)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyBCEt9anQMS"
      },
      "outputs": [],
      "source": [
        "input_file = open(\"train2id.txt\", \"r\")\n",
        "output_file = open(\"output.txt\", \"w\")\n",
        "\n",
        "# total number of lines\n",
        "number = int(input_file.readline())\n",
        "\n",
        "nodes = set()\n",
        "\n",
        "graph = {}\n",
        "\n",
        "for i in range(number):\n",
        "    content = input_file.readline()\n",
        "    node1, node2, relation = content.strip().split()\n",
        "\n",
        "    nodes.add(node1)\n",
        "    # nodes.add(node2)\n",
        "\n",
        "    relation = int(relation)\n",
        "\n",
        "    # Check if the first node already exists in the dictionary\n",
        "    if node1 not in graph:\n",
        "        # If not, create a new dictionary for the node\n",
        "        graph[node1] = {}\n",
        "    # Add the neighboring node and the relationship to the dictionary for node1\n",
        "    graph[node1][node2] = relation\n",
        "\n",
        "\n",
        "node_list = list(nodes)\n",
        "node_list2 = list(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guEg-YCzVQh1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "# relation2id = {}\n",
        "\n",
        "# with open(\"relation2id.txt\", \"r\") as file:\n",
        "#     relations = int(file.readline())\n",
        "#     for line in file:\n",
        "#         relation, relation_id = line.strip().split(\"\\t\")\n",
        "#         relation2id[int(relation_id)] = relation\n",
        "\n",
        "unique_rows = set()\n",
        "\n",
        "# change size if you want, but larger the size, smaller the data\n",
        "size = 30\n",
        "# how many positive and negative data set you want to train, the more dataset, the more time to train\n",
        "total = 60000\n",
        "fieldnames = ['Prompt', 'input_text', 'output_text']\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "\n",
        "with open(\"train_data.csv\", mode=\"w\", newline='') as tra:\n",
        "  with open(\"positive_data.csv\", mode=\"w\", newline='') as pos:\n",
        "    with open(\"negative_data.csv\",  mode=\"w\", newline='') as neg:\n",
        "\n",
        "        # Create a CSV writer object and write the headers to the file\n",
        "        writer_pos = csv.DictWriter(pos, fieldnames=fieldnames)\n",
        "        writer_pos.writeheader()\n",
        "\n",
        "        writer_neg = csv.DictWriter(neg, fieldnames=fieldnames)\n",
        "        writer_neg.writeheader()\n",
        "\n",
        "        writer_tra = csv.DictWriter(tra, fieldnames=fieldnames)\n",
        "        writer_tra.writeheader()\n",
        "\n",
        "\n",
        "        def dfs(graph, size):\n",
        "            pos_count = 0\n",
        "            neg_count = 0\n",
        "            times = 0\n",
        "            term = True\n",
        "\n",
        "            while times < total:\n",
        "                visited = set()\n",
        "                kg = []\n",
        "                graph_size = random.randint(2, size)\n",
        "                first_node = random.choice(node_list)\n",
        "                visited.add(first_node)\n",
        "                last_node = \"\"\n",
        "                previous_node = first_node\n",
        "                stack = [first_node]\n",
        "                input_text = \"\"\n",
        "                output_text = \"\"\n",
        "                while len(visited) < graph_size:\n",
        "                    if previous_node not in graph or set(graph[previous_node].keys()).issubset(visited):\n",
        "                        node = random.choice(node_list)\n",
        "                        while node in visited:\n",
        "                            node = random.choice(node_list)\n",
        "                        input_text += 'node_{} not connected with node_{}. '.format(previous_node, node)\n",
        "                        # output_text += 'node_{} not connected with node_{} means there is no relationship between node_{} and node_{}. '.format(previous_node, node, previous_node, node)\n",
        "                        visited.add(node)\n",
        "                        previous_node = node\n",
        "                    else:\n",
        "                        node = random.choice(list(graph[previous_node].keys()))\n",
        "                        while node in visited:\n",
        "                            node = random.choice(list(graph[previous_node].keys()))\n",
        "                        relation = graph[previous_node][node]\n",
        "                        # r = relation2id[relation]\n",
        "                        input_text += 'node_{} has relation_{} with node_{}. '.format(previous_node, relation, node)\n",
        "                        # output_text += 'node_{} has relation_{} with node_{}, means node_{} {} node_{}. '.format(previous_node, relation, node, previous_node, r, node)\n",
        "                        visited.add(node)\n",
        "                        previous_node = node\n",
        "                    if len(visited) == graph_size:\n",
        "                        last_node = previous_node\n",
        "\n",
        "                # input_text += 'Answer the following yes/no question by reasoning step-by-step. Is the first node connnected with the last node?'\n",
        "                was = len(unique_rows)\n",
        "                unique_rows.add(input_text)\n",
        "                if len(unique_rows) > was:\n",
        "                  # if pos_count < int(total/2):\n",
        "                    if first_node in graph and last_node in graph[first_node] and term:\n",
        "                      # if pos_count < int(total/2):\n",
        "                        # final_relation = relation2id[graph[first_node][last_node]]\n",
        "                        output_text += 'The answer is yes.'\n",
        "                        prompt = 'Is node {} connnected with node {}?'.format(first_node, last_node)\n",
        "                        comb = \"###Input:\\n\" + input_text + prompt + \"\\n\\n###Response:\\n\" + output_text\n",
        "                        writer_pos.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        pos_count += 1\n",
        "                        term = False\n",
        "                        times += 1\n",
        "                    elif last_node in graph and first_node in graph[last_node] and term:\n",
        "                      # if pos_count < int(total/2):\n",
        "                        # final_relation = relation2id[graph[last_node][first_node]]\n",
        "                        output_text += 'The answer is yes.'\n",
        "                        prompt = 'Is node {} connected with node {}?'.format(last_node, first_node)\n",
        "                        comb = \"###Input:\\n\" + input_text + prompt + \"\\n\\n###Response:\\n\" + output_text\n",
        "                        writer_pos.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        pos_count += 1\n",
        "                        term = False\n",
        "                        times += 1\n",
        "                  # elif neg_count < int(total//2):\n",
        "                    elif not term:\n",
        "                      # if neg_count < int(total/2):\n",
        "                        output_text += 'The answer is no.'\n",
        "                        prompt = 'Is node {} connected with node {}?'.format(first_node, last_node)\n",
        "                        comb = \"###Input:\\n\" + input_text + prompt + \"\\n\\n###Response:\\n\" + output_text\n",
        "                        writer_neg.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'Prompt': comb, 'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        neg_count += 1\n",
        "                        term = True\n",
        "                        times += 1\n",
        "                else:\n",
        "                  continue\n",
        "\n",
        "\n",
        "            print(pos_count)\n",
        "            print(neg_count)\n",
        "\n",
        "        dfs(graph, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXwHTTqWCEvX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 读取CSV文件\n",
        "positive_df = pd.read_csv('positive_data.csv')\n",
        "negative_df = pd.read_csv('negative_data.csv')\n",
        "\n",
        "# 各自分割数据集为80%的训练集和20%的测试集\n",
        "train_pos, test_pos = train_test_split(positive_df, test_size=0.2, random_state=42)\n",
        "train_neg, test_neg = train_test_split(negative_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# 合并训练集和测试集\n",
        "train_df = pd.concat([train_pos, train_neg])\n",
        "test_df = pd.concat([test_pos, test_neg])\n",
        "\n",
        "# 打乱训练集和测试集\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfkOriPen9YQ"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "#TODO:\n",
        "# !huggingface-cli login\n",
        "login(token = \"\")\n",
        "\n",
        "\n",
        "def load_model(model_name, bnb_config):\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{40960}MB'\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
        "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def preprocess_batch(batch, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        batch[\"Prompt\"],\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_bnb_config():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    return bnb_config\n",
        "\n",
        "def create_peft_config(modules):\n",
        "    \"\"\"\n",
        "    Create Parameter-Efficient Fine-Tuning config for your model\n",
        "    :param modules: Names of the modules to apply Lora to\n",
        "    \"\"\"\n",
        "    config = LoraConfig(\n",
        "        r=16,  # dimension of the updated matrices\n",
        "        lora_alpha=64,  # parameter for scaling\n",
        "        target_modules=modules,\n",
        "        lora_dropout=0.1,  # dropout probability for layers\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model, use_4bit=False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "        # if using DS Zero 3 and the weights are initialized empty\n",
        "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "            num_params = param.ds_numel\n",
        "\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "    if use_4bit:\n",
        "        trainable_params /= 2\n",
        "    print(\n",
        "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "def train(model, tokenizer, dataset, output_dir):\n",
        "    # Apply preprocessing to the model to prepare it by\n",
        "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Get lora module names\n",
        "    modules = find_all_linear_names(model)\n",
        "\n",
        "    # Create PEFT config for these modules and wrap the model to PEFT\n",
        "    peft_config = create_peft_config(modules)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print information about the percentage of trainable parameters\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Training parameters\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=2,\n",
        "            max_steps=15,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            logging_steps=1,\n",
        "            output_dir=\"outputs\",\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "        ),\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
        "\n",
        "\n",
        "    dtypes = {}\n",
        "    for _, p in model.named_parameters():\n",
        "        dtype = p.dtype\n",
        "        if dtype not in dtypes: dtypes[dtype] = 0\n",
        "        dtypes[dtype] += p.numel()\n",
        "    total = 0\n",
        "    for k, v in dtypes.items(): total+= v\n",
        "    for k, v in dtypes.items():\n",
        "        print(k, v, v/total)\n",
        "\n",
        "    do_train = True\n",
        "\n",
        "    # Launch training\n",
        "    print(\"Training...\")\n",
        "\n",
        "    if do_train:\n",
        "        train_result = trainer.train()\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        print(metrics)\n",
        "\n",
        "    ###\n",
        "\n",
        "    # Saving model\n",
        "    print(\"Saving last checkpoint of the model...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    trainer.model.save_pretrained(output_dir)\n",
        "\n",
        "    # Free memory for merging weights\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# TODO 这里file_path改成自己的csv 确保你的数据在Prompt的column里\n",
        "def main(file_path = \"/content/train.csv\", output_dir = \"final_checkpoint\"):\n",
        "    #Models that need to be fine-tuned\n",
        "    model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "    bnb_config = create_bnb_config()\n",
        "    model, tokenizer = load_model(model_name, bnb_config)\n",
        "    df = pd.read_csv(file_path)\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    _preprocessing_function = partial(preprocess_batch, tokenizer=tokenizer)\n",
        "\n",
        "    dataset = dataset.map(\n",
        "            _preprocessing_function,\n",
        "            batched=True,\n",
        "        )\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "    output_dir = output_dir\n",
        "    train(model, tokenizer, dataset, output_dir)\n",
        "\n",
        "main(\"/content/train.csv\", \"final_checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXVakEhrtYBP"
      },
      "outputs": [],
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(\"final_checkpoint\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "output_merged_dir = \"results/llama2/final_merged_checkpoint\"\n",
        "os.makedirs(output_merged_dir, exist_ok=True)\n",
        "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
        "\n",
        "# save tokenizer for easy inference\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVbKvY1a0vp"
      },
      "source": [
        "Link prediction without ICL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2i2UcUPtYBV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 加载CSV文件\n",
        "csv_file = 'test.csv'  # 请替换为你的CSV文件名\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 在所有的'input_text'数据前面加上\"###Input:\"\n",
        "df['input_text'] = \"###Input: \\n\" + df['input_text']\n",
        "\n",
        "# 保存更改后的DataFrame到新的CSV文件\n",
        "df.to_csv('modified_' + csv_file, index=False)\n",
        "\n",
        "print(\"Updated CSV file has been saved as 'modified_\" + csv_file + \"'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPpVDlfmtusY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 读取CSV文件\n",
        "csv_file = '/content/modified_test.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 限制测试数量\n",
        "test_limit = 2000\n",
        "# 计数器\n",
        "accurate_count = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if index <= test_limit:  # 检查是否已达到测试数量的限制\n",
        "          break\n",
        "    # 获取输入文本\n",
        "    input_text = row['input_text']\n",
        "\n",
        "    # 使用模型生成回答\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
        "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # 检查模型回答和期望输出中是否包含'yes'或'Yes'\n",
        "    model_has_yes = 'yes' in model_answer.lower()\n",
        "    expected_has_yes = 'yes' in row['output_text'].lower()\n",
        "\n",
        "    # print(\"model_has_yes: \", model_has_yes)\n",
        "    # print(\"expected_has_yes: \", expected_has_yes)\n",
        "\n",
        "    y_true.append(expected_has_yes)\n",
        "    y_pred.append(model_has_yes)\n",
        "\n",
        "\n",
        "    # 判断准确性\n",
        "    if model_has_yes == expected_has_yes:\n",
        "        accurate_count += 1\n",
        "        print(accurate_count)\n",
        "\n",
        "    print(index)\n",
        "\n",
        "# 计算准确率\n",
        "f1 = f1_score(y_true, y_pred, pos_label=True)\n",
        "accuracy = accurate_count / min(len(df), test_limit)\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QyiaBcsa7Xt"
      },
      "source": [
        "Link prediction with ICL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8sZQsaUvQyl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('test.csv')\n",
        "count = len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "563bAS-ZvQym"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"train.csv\") as train:\n",
        "  with open(\"context.csv\", mode=\"w\", newline='') as context:\n",
        "      writer_context = csv.DictWriter(context, fieldnames=fieldnames)\n",
        "      writer_context.writeheader()\n",
        "      reader_train = csv.reader(train)\n",
        "      # Iterate over each row in the CSV file\n",
        "      for row in reader_train:\n",
        "          input = row[1]\n",
        "          output = row[2]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "\n",
        "          input = input.split('.')\n",
        "          input = input[:-1]\n",
        "          input = '.'.join(input)\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3:-1]\n",
        "          # relationship = words[3]\n",
        "          # print(words)\n",
        "          words = '.'.join(words)\n",
        "\n",
        "          if c < count and len(input) < 30:\n",
        "            cont = '###Context: {}.\\n\\n###Input:'.format(input + '.' + words)\n",
        "            writer_context.writerow({'input_text': cont, 'output_text': words})\n",
        "            c += 1\n",
        "            # print(c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6gpm7VVvQym"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# 加载CSV文件\n",
        "context_df = pd.read_csv('context.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# 对test.csv中的每个input_text随机添加一个context.csv中的input_text\n",
        "for index, row in test_df.iterrows():\n",
        "    # 随机选择一个context\n",
        "    random_context = random.choice(context_df['input_text'])\n",
        "    # 将context和test中的文本合并，并直接覆盖原有的input_text\n",
        "    test_df.at[index, 'input_text'] = random_context + \" \" + row['input_text']\n",
        "\n",
        "# 保存更新后的test_df到CSV文件\n",
        "test_df.to_csv('augmented_test.csv', index=False)\n",
        "\n",
        "print(\"Augmented test CSV file has been created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oyVQlWXvQym"
      },
      "outputs": [],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('augmented_test.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDzCb-s1vQym"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 读取CSV文件\n",
        "csv_file = '/content/augmented_test.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 限制测试数量\n",
        "test_limit = 2000\n",
        "# 计数器\n",
        "accurate_count = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if index <= test_limit:  # 检查是否已达到测试数量的限制\n",
        "          break\n",
        "    # 获取输入文本\n",
        "    input_text = row['input_text']\n",
        "\n",
        "    # 使用模型生成回答\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
        "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # 检查模型回答和期望输出中是否包含'yes'或'Yes'\n",
        "    model_has_yes = 'yes' in model_answer.lower()\n",
        "    expected_has_yes = 'yes' in row['output_text'].lower()\n",
        "\n",
        "    # print(\"model_has_yes: \", model_has_yes)\n",
        "    # print(\"expected_has_yes: \", expected_has_yes)\n",
        "\n",
        "    y_true.append(expected_has_yes)\n",
        "    y_pred.append(model_has_yes)\n",
        "\n",
        "\n",
        "    # 判断准确性\n",
        "    if model_has_yes == expected_has_yes:\n",
        "        accurate_count += 1\n",
        "        print(accurate_count)\n",
        "\n",
        "    print(index)\n",
        "\n",
        "# 计算准确率\n",
        "f1 = f1_score(y_true, y_pred, pos_label=True)\n",
        "accuracy = accurate_count / min(len(df), test_limit)\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNzyZYy8a99N"
      },
      "source": [
        "Relation prediction without ICL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruppW9s2vQym"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "# context = 'Given node_2075 has relation_4 with node_12648. The relation between the first node and last node is _member_meronym. '\n",
        "# instruction = 'Answer the following multiple-choice question by choosing one of these options: '\n",
        "# option = ''\n",
        "instruction = 'Answer the following question step-by-step. '\n",
        "count = 0\n",
        "\n",
        "question = 'The relationship between the first node and the last node is ?'\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"relation2id.txt\", \"r\") as file:\n",
        "    relations = int(file.readline())\n",
        "    for line in file:\n",
        "        relation, relation_id = line.strip().split(\"\\t\")\n",
        "    #     instruction += relation + ', '\n",
        "    # instruction = instruction[:-2] + '. '\n",
        "\n",
        "with open(\"test.csv\") as test:\n",
        "    with open(\"relation.csv\", mode=\"w\", newline='') as icl:\n",
        "      writer_icl = csv.DictWriter(icl, fieldnames=fieldnames)\n",
        "      writer_icl.writeheader()\n",
        "      reader = csv.reader(test)\n",
        "\n",
        "\n",
        "      for row in reader:\n",
        "          # print(row)\n",
        "          input = row[1]\n",
        "          output = row[2]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "          input = input.split('.')\n",
        "          input = input[:-1]\n",
        "          input = '.'.join(input)\n",
        "          # print(input)\n",
        "          input = instruction + input + '. ' + question\n",
        "          # input = input + '. ' + instruction + question\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "          # print(relationship)\n",
        "          output = output[:-2]\n",
        "          output = '.'.join(output)\n",
        "          # print(output)\n",
        "          output += '. The relationship between the first node and the last node is {}.'.format(relationship)\n",
        "          # print(output)\n",
        "          if len(input) < 550:\n",
        "            writer_icl.writerow({'input_text': input, 'output_text': output})\n",
        "            count+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUt9Wy7RvQym"
      },
      "outputs": [],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('relation.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug5t5yO6vQyn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "csv_file = '/content/relation.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 初始化计数器和结果列表\n",
        "tp = fp = fn = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "test_limit = 500\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if index <= test_limit:  # 检查是否已达到测试数量的限制\n",
        "        break\n",
        "    input_text = row['input_text']\n",
        "    expected_word = row['output_text'].rstrip('.').split()[-1].lower()\n",
        "\n",
        "    # 使用模型生成回答\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
        "\n",
        "    # 检查模型回答中是否包含最后一个单词\n",
        "    contains_word = expected_word in model_answer.rstrip('.').split()\n",
        "\n",
        "    # 更新计数器\n",
        "    if contains_word:\n",
        "        tp += 1\n",
        "        y_pred.append(1)\n",
        "    else:\n",
        "        fn += 1\n",
        "        y_pred.append(0)\n",
        "    y_true.append(1)\n",
        "\n",
        "# 计算准确度和F1分数\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMy1MwumbDFb"
      },
      "source": [
        "Relation prediction with ICL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-28Uv0n-vQyn"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "# context = 'Given node_2075 has relation_4 with node_12648. The relation between the first node and last node is _member_meronym. '\n",
        "# instruction = 'Answer the following multiple-choice question by choosing one of these options: '\n",
        "# option = ''\n",
        "instruction = 'Answer the following question step-by-step. '\n",
        "count = 0\n",
        "\n",
        "question = 'The relationship between the first node and the last node is ?'\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"relation2id.txt\", \"r\") as file:\n",
        "    relations = int(file.readline())\n",
        "    for line in file:\n",
        "        relation, relation_id = line.strip().split(\"\\t\")\n",
        "    #     instruction += relation + ', '\n",
        "    # instruction = instruction[:-2] + '. '\n",
        "\n",
        "with open(\"test.csv\") as test:\n",
        "    with open(\"test_icl.csv\", mode=\"w\", newline='') as icl:\n",
        "      writer_icl = csv.DictWriter(icl, fieldnames=fieldnames)\n",
        "      writer_icl.writeheader()\n",
        "      reader = csv.reader(test)\n",
        "\n",
        "\n",
        "      for row in reader:\n",
        "          # print(row)\n",
        "          input = row[1]\n",
        "          output = row[2]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "          input = input.split('.')\n",
        "          input = input[:-1]\n",
        "          input = '.'.join(input)\n",
        "          # print(input)\n",
        "          input = instruction + input + '. ' + question\n",
        "          # input = input + '. ' + instruction + question\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "          # print(relationship)\n",
        "          output = output[:-2]\n",
        "          output = '.'.join(output)\n",
        "          # print(output)\n",
        "          output += '. The relationship between the first node and the last node is {}.'.format(relationship)\n",
        "          # print(output)\n",
        "          if len(input) < 550:\n",
        "            writer_icl.writerow({'input_text': input, 'output_text': output})\n",
        "            count+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqRpTNIXvQyn"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "\n",
        "with open(\"train.csv\") as train:\n",
        "  with open(\"context.csv\", mode=\"w\", newline='') as context:\n",
        "      writer_context = csv.DictWriter(context, fieldnames=fieldnames)\n",
        "      writer_context.writeheader()\n",
        "      reader_train = csv.reader(train)\n",
        "      # Iterate over each row in the CSV file\n",
        "      for row in reader_train:\n",
        "          input = row[1]\n",
        "          output = row[2]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "\n",
        "          input = input.split('.')\n",
        "          input = input[:-1]\n",
        "          input = '.'.join(input)\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "\n",
        "          if c < count and len(input) < 50:\n",
        "            cont = '###Context: {}. The relationship between the first node and last node is {}.\\n\\n###Input:'.format(input, relationship)\n",
        "            writer_context.writerow({'input_text': cont, 'output_text': relationship})\n",
        "            c += 1\n",
        "            # print(c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chzIA626vQyn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# 加载CSV文件\n",
        "context_df = pd.read_csv('context.csv')\n",
        "test_df = pd.read_csv('test_icl.csv')\n",
        "\n",
        "# 对test.csv中的每个input_text随机添加一个context.csv中的input_text\n",
        "for index, row in test_df.iterrows():\n",
        "    # 随机选择一个context\n",
        "    random_context = random.choice(context_df['input_text'])\n",
        "    # 将context和test中的文本合并，并直接覆盖原有的input_text\n",
        "    test_df.at[index, 'input_text'] = random_context + \" \" + row['input_text']\n",
        "\n",
        "# 保存更新后的test_df到CSV文件\n",
        "test_df.to_csv('icl_relation.csv', index=False)\n",
        "\n",
        "print(\"Augmented test CSV file has been created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqX0FdrvQyn"
      },
      "outputs": [],
      "source": [
        "# read csv files\n",
        "df1 = pd.read_csv('context.csv')\n",
        "df2 = pd.read_csv('test_icl.csv')\n",
        "\n",
        "# combine dataframes by interleaving rows\n",
        "\n",
        "combined_df = pd.concat([df1, df2]).sort_index(kind='merge')\n",
        "\n",
        "# write to new csv file\n",
        "combined_df.to_csv('icl_relation.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZiXu35MvQyn"
      },
      "outputs": [],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('icl_relation.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTV0CF3fvQyn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "csv_file = '/content/icl_relation.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 初始化计数器和结果列表\n",
        "tp = fp = fn = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "test_limit = 500\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if index <= test_limit:  # 检查是否已达到测试数量的限制\n",
        "        break\n",
        "    input_text = row['input_text']\n",
        "    expected_word = row['output_text'].rstrip('.').split()[-1].lower()\n",
        "\n",
        "    # 使用模型生成回答\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
        "\n",
        "    # 检查模型回答中是否包含最后一个单词\n",
        "    contains_word = expected_word in model_answer.rstrip('.').split()\n",
        "\n",
        "    # 更新计数器\n",
        "    if contains_word:\n",
        "        tp += 1\n",
        "        y_pred.append(1)\n",
        "    else:\n",
        "        fn += 1\n",
        "        y_pred.append(0)\n",
        "    y_true.append(1)\n",
        "\n",
        "# 计算准确度和F1分数\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
